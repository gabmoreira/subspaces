{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd98feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "import torch\n",
    "from torchtnt.utils.device import copy_data_to_device\n",
    "from torchvision.transforms.functional import resize, normalize\n",
    "import clip\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import average_precision_score\n",
    "import logging\n",
    "\n",
    "from model import Model\n",
    "from dataset import *\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "dataset_name = \"CELEBA\"\n",
    "root = \"./experiments/celeba_linear\"\n",
    "device = \"cuda:0\"\n",
    "\n",
    "with open(f\"{root}/.hydra/config.yaml\", \"r\") as f:\n",
    "    cfg = OmegaConf.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa5d332",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = get_loader(dataset_name=dataset_name,\n",
    "                         split=\"train\",\n",
    "                         img_size=cfg.general.img_size,\n",
    "                         batch_size=cfg.train.batch_size,\n",
    "                         num_workers=cfg.train.num_workers,\n",
    "                         pin_memory=cfg.train.pin_memory,\n",
    "                         persistent_workers=cfg.val.persistent_workers,)\n",
    "\n",
    "testloader = get_loader(dataset_name=dataset_name,\n",
    "                        split=\"test\",\n",
    "                        img_size=cfg.general.img_size,\n",
    "                        batch_size=cfg.val.batch_size,\n",
    "                        num_workers=cfg.train.num_workers,\n",
    "                        pin_memory=cfg.train.pin_memory,\n",
    "                        persistent_workers=cfg.val.persistent_workers,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcaa832",
   "metadata": {},
   "source": [
    "# Load CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eabb4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model, clip_preprocess = clip.load(\"ViT-B/32\");\n",
    "clip_model.to(device).eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751ec05c",
   "metadata": {},
   "source": [
    "# Load our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fc5d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "module = Model(\n",
    "    dim_in=cfg.model.dim_in,\n",
    "    dim_out=cfg.model.dim_out,\n",
    "    backbone=cfg.model.backbone,\n",
    "    kernel=cfg.model.kernel,\n",
    "    activation=torch.nn.functional.leaky_relu,\n",
    "    alpha=cfg.train.loss_coefs.alpha,\n",
    "    beta=cfg.train.loss_coefs.beta,\n",
    "    device=device,\n",
    ").to(device)\n",
    "\n",
    "state_dict = torch.load(os.path.join(root, \"state_dict.pt\"), map_location=\"cpu\", weights_only=True)\n",
    "module.load_state_dict(state_dict)\n",
    "module.eval()\n",
    "\n",
    "module.forget()\n",
    "for data in trainloader:\n",
    "    data = copy_data_to_device(data, device=device)\n",
    "    with torch.no_grad():\n",
    "        output = module(data)\n",
    "module.update_minterms()\n",
    "\n",
    "minterms = []\n",
    "minterm_evecs = []\n",
    "for k,v in module._minterms.items():\n",
    "    minterms.append(k)\n",
    "    minterm_evecs.append(v)\n",
    "minterms = torch.tensor(minterms).to(device)\n",
    "minterm_evecs = torch.cat(minterm_evecs).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d811da",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "sns.heatmap(module.kernel(minterm_evecs, minterm_evecs).cpu() ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aff1945",
   "metadata": {},
   "source": [
    "# Embed test set with our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab577846",
   "metadata": {},
   "outputs": [],
   "source": [
    "our_test_embeddings = []\n",
    "test_images = []\n",
    "test_labels = []\n",
    "for data in testloader:\n",
    "    data = copy_data_to_device(data, device=device)\n",
    "    test_images.append(data.images)\n",
    "    test_labels.append(data.labels)\n",
    "    with torch.no_grad():\n",
    "        our_test_embeddings.append(module.embed(data)[0])\n",
    "our_test_embeddings = torch.cat(our_test_embeddings, dim=0)\n",
    "test_images = torch.cat(test_images, dim=0)\n",
    "test_labels = torch.cat(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524476f4",
   "metadata": {},
   "source": [
    "# Embed test set with CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806e7a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_test_embeddings = []\n",
    "for data in testloader:\n",
    "    data = copy_data_to_device(data, device=device)\n",
    "    with torch.no_grad():\n",
    "        preprocessed = resize(data.images, 224)\n",
    "        preprocessed = normalize(preprocessed,\n",
    "                                 mean=(0.48145466, 0.4578275, 0.40821073),\n",
    "                                 std=(0.26862954, 0.26130258, 0.27577711))\n",
    "        clip_test_embeddings.append(clip_model.encode_image(preprocessed).float())\n",
    "clip_test_embeddings = torch.cat(clip_test_embeddings, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e8df1a",
   "metadata": {},
   "source": [
    "# Retrieval (all queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e65cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words = True\n",
    "with open(\"./queries.txt\") as file:\n",
    "    lines = [line.rstrip() for line in file]\n",
    "    \n",
    "queries_ = [l.split(\"-\")[0].strip() for l in lines]\n",
    "nl_queries_ = [l.split(\"-\")[1].strip() for l in lines]\n",
    "\n",
    "our_ap = []\n",
    "our_pr = []\n",
    "clip_ap = []\n",
    "clip_pr = []\n",
    "queries = []\n",
    "nl_queries = []\n",
    "\n",
    "for query, nl_query in zip(queries_, nl_queries_):\n",
    "    if bag_of_words:\n",
    "        nl_query = query.replace(\" and\", \", \").replace(\"not\", \"\").replace(\"_\", \" \").strip()\n",
    "        \n",
    "    print(f\"\\n{query}\\n{nl_query}\")\n",
    "\n",
    "    literals = [l.strip() for l in query.split(\"and\")]\n",
    "    pos_literals = [l for l in literals if \"not\" not in l]\n",
    "    neg_literals = [l.replace(\"not\", \"\").strip() for l in literals if \"not\" in l]\n",
    "    pos_idx = [testloader.dataset.class_to_idx[l] for l in pos_literals]\n",
    "    neg_idx = [testloader.dataset.class_to_idx[l] for l in neg_literals]\n",
    "\n",
    "    target = torch.logical_and((test_labels[:,pos_idx] == 1).all(-1), (test_labels[:,neg_idx] == 0).all(-1))\n",
    "\n",
    "    if target.sum() < 10:\n",
    "        continue\n",
    "\n",
    "    queries.append(query)\n",
    "    nl_queries.append(nl_query)\n",
    "\n",
    "    masks = [minterms[:,idx] == 1 for idx in pos_idx]\n",
    "    masks.extend([minterms[:,idx] == 0 for idx in neg_idx])\n",
    "\n",
    "    mask = masks[0]\n",
    "    for i in range(len(masks)):\n",
    "        mask = torch.logical_and(mask, masks[i])\n",
    "        \n",
    "    projection = minterm_evecs[mask].T @ minterm_evecs[mask]\n",
    "\n",
    "    p = torch.einsum(\n",
    "        \"bi,ij,bj->b\",\n",
    "        F.normalize(our_test_embeddings, dim=-1, p=2),\n",
    "        projection,\n",
    "        F.normalize(our_test_embeddings, dim=-1, p=2)\n",
    "    )\n",
    "    idx = torch.argsort(p, descending=True)\n",
    "\n",
    "    ap = average_precision_score(target.cpu(), p.cpu())\n",
    "    precision = (target[idx[:10]].sum() / 10).cpu().item()\n",
    "    print(f\"Our AP = {ap}\")\n",
    "    print(f\"Our Pr@10 = {precision}\")\n",
    "    our_ap.append(ap)\n",
    "    our_pr.append(precision)\n",
    "\n",
    "    #CLIP\n",
    "    with torch.no_grad():\n",
    "        text_tokens = clip.tokenize([nl_query]).to(device)\n",
    "        clip_text_embedding = clip_model.encode_text(text_tokens).float()\n",
    "\n",
    "    clip_test_embeddings /= clip_test_embeddings.norm(dim=-1, keepdim=True)\n",
    "    clip_text_embedding /= clip_text_embedding.norm(dim=-1, keepdim=True)\n",
    "    similarity = (clip_text_embedding @ clip_test_embeddings.T).squeeze()\n",
    "    idx = torch.argsort(similarity.squeeze(), descending=True) \n",
    "\n",
    "    ap = average_precision_score(target.cpu(), similarity.cpu())\n",
    "    precision = (target[idx[:10]].sum() / 10).cpu().item()\n",
    "    print(f\"CLIP AP = {ap}\")\n",
    "    print(f\"CLIP Pr@10 = {precision}\")\n",
    "    clip_ap.append(ap)\n",
    "    clip_pr.append(precision)\n",
    "    \n",
    "print(\"-\"*50)\n",
    "print(f\"Pr@10 positive (Ours) = {np.array(our_pr)[np.array(['not' not in q for q in queries])].mean()}\")\n",
    "print(f\"mAP   positive (Ours) = {np.array(our_ap)[np.array(['not' not in q for q in queries])].mean()}\")\n",
    "print(f\"Pr@10 negative (Ours) = {np.array(our_pr)[np.array(['not' in q for q in queries])].mean()}\")\n",
    "print(f\"mAP   negative (Ours) = {np.array(our_ap)[np.array(['not' in q for q in queries])].mean()}\")\n",
    "print(\"-\"*50)\n",
    "print(f\"Pr@10 positive (CLIP) = {np.array(clip_pr)[np.array(['not' not in q for q in queries])].mean()}\")\n",
    "print(f\"mAP   positive (CLIP) = {np.array(clip_ap)[np.array(['not' not in q for q in queries])].mean()}\")\n",
    "print(f\"Pr@10 negative (CLIP) = {np.array(clip_pr)[np.array(['not' in q for q in queries])].mean()}\")\n",
    "print(f\"mAP   negative (CLIP) = {np.array(clip_ap)[np.array(['not' in q for q in queries])].mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8b7b7e",
   "metadata": {},
   "source": [
    "# Retrieval visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e21dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Bald\"\n",
    "nl_query = \"a male person that is not bald\"\n",
    "\n",
    "literals = [l.strip() for l in query.split(\"and\")]\n",
    "\n",
    "pos_literals = [l for l in literals if \"not\" not in l]\n",
    "neg_literals = [l.replace(\"not\", \"\").strip() for l in literals if \"not\" in l]\n",
    "\n",
    "print(f\"Positive literals {pos_literals}\")\n",
    "print(f\"Negative literals {neg_literals}\")\n",
    "\n",
    "pos_idx = [testloader.dataset.class_to_idx[l] for l in pos_literals]\n",
    "neg_idx = [testloader.dataset.class_to_idx[l] for l in neg_literals]\n",
    "\n",
    "target = torch.logical_and(\n",
    "    (test_labels[:,pos_idx] == 1).all(-1),\n",
    "    (test_labels[:,neg_idx] == 0).all(-1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cae4806",
   "metadata": {},
   "outputs": [],
   "source": [
    "masks = [minterms[:,idx] == 1 for idx in pos_idx]\n",
    "masks.extend([minterms[:,idx] == 0 for idx in neg_idx])\n",
    "mask = masks[0]\n",
    "for i in range(len(masks)):\n",
    "    mask = torch.logical_and(mask, masks[i])\n",
    "    \n",
    "projection = minterm_evecs[mask].T @ minterm_evecs[mask]\n",
    "u, s, vt = torch.linalg.svd(projection)\n",
    "print(s)\n",
    "\n",
    "p = torch.einsum(\n",
    "    \"bi,ij,bj->b\",\n",
    "    F.normalize(our_test_embeddings, dim=-1, p=2),\n",
    "    projection,\n",
    "    F.normalize(our_test_embeddings, dim=-1, p=2)\n",
    ")\n",
    "idx = torch.argsort(p, descending=True)\n",
    "\n",
    "ap = average_precision_score(target.cpu(), p.cpu())\n",
    "precision = target[idx[:10]].sum() / 10\n",
    "print(f\"AP = {ap}\")\n",
    "print(f\"Precision@10 = {precision}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8365a74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 20\n",
    "fig = plt.figure(figsize=(20,5))\n",
    "for i in range(n):\n",
    "    plt.subplot(2,n//2,i+1)\n",
    "    plt.imshow(test_images[idx[i]].permute(1,2,0).cpu())\n",
    "    plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "#fig.savefig(f\"{query}_ours.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8966665d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    text_tokens = clip.tokenize([nl_query]).to(device)\n",
    "    clip_text_embedding = clip_model.encode_text(text_tokens).float()\n",
    "\n",
    "clip_test_embeddings /= clip_test_embeddings.norm(dim=-1, keepdim=True)\n",
    "clip_text_embedding /= clip_text_embedding.norm(dim=-1, keepdim=True)\n",
    "similarity = (clip_text_embedding @ clip_test_embeddings.T).squeeze()\n",
    "idx = torch.argsort(similarity.squeeze(), descending=True) \n",
    "\n",
    "ap = average_precision_score(target.cpu(), similarity.cpu())\n",
    "precision = target[idx[:10]].sum() / 10\n",
    "print(f\"AP = {ap}\")\n",
    "print(f\"Precision@10 = {precision}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a3d84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 20\n",
    "fig = plt.figure(figsize=(20,5))\n",
    "for i in range(n):\n",
    "    plt.subplot(2,n//2,i+1)\n",
    "    plt.imshow(test_images[idx[i]].permute(1,2,0).cpu())\n",
    "    plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "#fig.savefig(f\"{nl_query}_clip.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
